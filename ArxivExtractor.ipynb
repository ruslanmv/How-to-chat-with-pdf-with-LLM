{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1382ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from rake_nltk import Rake\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\"\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases\n",
    "\n",
    "noun_phrases = get_main_topic(question)\n",
    "\n",
    "# Convert noun_phrases to a Python list\n",
    "noun_phrases_list = list(noun_phrases)\n",
    "print(noun_phrases_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95288818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant keyword for search: effective treatments cardiovascular\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "# Sample list of noun phrases\n",
    "#noun_phrases_list = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "# Join the noun phrases to form a single string\n",
    "text = ' '.join(noun_phrases_list)\n",
    "\n",
    "# Creating a YAKE keyword extractor\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "# Extract keywords using YAKE\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "# Considering the most relevant keyword (the first in the list) as per YAKE\n",
    "if keywords:\n",
    "    most_relevant_keyword = keywords[0][0]\n",
    "    print(\"Most relevant keyword for search:\", most_relevant_keyword)\n",
    "else:\n",
    "    print(\"No keywords extracted from the given text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b9fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce23a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant keyword for search: effective treatments cardiovascular diseases\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "# Sample list of noun phrases\n",
    "noun_phrases_list = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "# Join the noun phrases to form a single string\n",
    "text = ' '.join(noun_phrases_list)\n",
    "\n",
    "# Specify parameters for YAKE\n",
    "language = \"en\"\n",
    "max_ngram_size = 4  # Considering single words as keywords\n",
    "deduplication_threshold = 0.9\n",
    "\n",
    "# Create a YAKE keyword extractor\n",
    "kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold)\n",
    "\n",
    "# Extract keywords\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "# Considering the most relevant keyword (first in the list) as per YAKE\n",
    "if keywords:\n",
    "    most_relevant_keyword = keywords[0][0]\n",
    "    print(\"Most relevant keyword for search:\", most_relevant_keyword)\n",
    "else:\n",
    "    print(\"No keywords extracted from the given text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9526d076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant keyword for search: effective treatments cardiovascular\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592b809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "664d149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant keyword for search: effective treatments cardiovascular diseases patients worldwide\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "# Sample list of noun phrases\n",
    "noun_phrases_list = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "# Join the noun phrases to form a single string\n",
    "text = ' '.join(noun_phrases_list)\n",
    "\n",
    "# Using Rake algorithm to extract keywords\n",
    "r = Rake()\n",
    "\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "# Get the ranked keywords from Rake\n",
    "ranked_keywords = r.get_ranked_phrases()\n",
    "\n",
    "# Considering the most relevant keyword (first in the list) as per Rake\n",
    "most_relevant_keyword = ranked_keywords[0]\n",
    "\n",
    "print(\"Most relevant keyword for search:\", most_relevant_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bd914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca56e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c973c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6a2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac908c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\.conda\\envs\\chatpdf\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01ab9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from rake_nltk import Rake\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\"\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b5c3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrases = get_main_topic(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4d3fb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['effective treatments', 'cardiovascular diseases', 'patients worldwide'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9825c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords using RAKE on the noun phrases\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(\" \".join(noun_phrases))  # Joining the noun phrases into a single string\n",
    "rake_keywords = r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fa669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effective treatments cardiovascular diseases patients worldwide']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2c0248",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract keywords using RAKE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m r \u001b[38;5;241m=\u001b[39m Rake()\n\u001b[1;32m----> 3\u001b[0m r\u001b[38;5;241m.\u001b[39mextract_keywords_from_text(\u001b[43msentence\u001b[49m)\n\u001b[0;32m      4\u001b[0m rake_keywords \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mget_ranked_phrases()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract keywords using RAKE\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(sentence)\n",
    "rake_keywords = r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6df97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4806c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Extract keywords using RAKE\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    rake_keywords = r.get_ranked_phrases()\n",
    "    \n",
    "    # Extract nouns and proper nouns using spaCy\n",
    "    spacy_keywords = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            spacy_keywords.append(token.text)\n",
    "    \n",
    "    return filtered_tokens, rake_keywords, spacy_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens, rake_keys, spacy_keys = extract_keywords(question)\n",
    "\n",
    "print(\"Filtered Tokens:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "print(\"\\nRAKE Keywords:\")\n",
    "print(rake_keys)\n",
    "\n",
    "print(\"\\nspaCy Keywords:\")\n",
    "print(spacy_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17182f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d3673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c90cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3874a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42794685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b001a5d1-5ded-4982-9210-2e6b1ddfd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c74af1-afbd-4d47-a195-ea25edc42ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0eb1c0-68b2-4d5f-9ee7-605b520680e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830433fd-3734-4f7f-b7e7-4a1f1a25077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_topic(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entities = [entity.text for entity in doc.ents]\n",
    "    return entities[0] if entities else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd47e26-f033-48b2-a5e8-65a7cfc6a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11204d3-464d-47c9-94b9-0d05dbbfec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "main_topic = get_main_topic(question)\n",
    "\n",
    "print(main_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7a138b-80a3-43a8-a76a-40f748211afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c43edaee-881e-45f4-9391-b42a16f40b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'the most effective treatments',\n",
       " 'cardiovascular diseases',\n",
       " 'they',\n",
       " 'patients']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_main_topic(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38662545-f344-4009-bc3f-4799f774bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92ceea0b-fc41-440f-a181-934a98337890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd76fb41-0a74-46d7-803d-568466f15cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist=get_main_topic(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "583a7f8c-ad4d-48ce-87be-82a2a3278f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effective treatments'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0314fb-b65d-4758-9166-598a50da21b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c706d18b-74e1-491e-9833-844c13886a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "043cf7d0-421c-45df-9680-d03309953498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    tokens = word_tokenize(words)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    filtered_tagged = [word for word in tagged if word[0].lower() not in stopwords_list]\n",
    "    \n",
    "    noun_list = [word[0] for word in filtered_tagged if word[1] == 'NN' or word[1] == 'NNS']\n",
    "    \n",
    "    count_nouns = Counter(noun_list)\n",
    "    main_noun = count_nouns.most_common(1)[0][0]\n",
    "    \n",
    "    return main_noun\n",
    "\n",
    "main_noun = get_main_nouns(wordlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adbcb59c-5549-4901-85c9-2bc4a7f68d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatments\n"
     ]
    }
   ],
   "source": [
    "print(main_noun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131edf7-1fd9-4d7b-a4ef-8ed27601ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7618ae0-936a-4368-abb8-c32831796e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wordlist = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    tokens = word_tokenize(words)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    filtered_tagged = [word for word in tagged if word[0].lower() not in stopwords_list]\n",
    "    \n",
    "    noun_list = [word[0] for word in filtered_tagged if word[1] == 'NN' or word[1] == 'NNS']\n",
    "    \n",
    "    # Count the frequency of each noun in the wordlist\n",
    "    count_nouns = {}\n",
    "    for noun in noun_list:\n",
    "        count_nouns[noun] = sum([phrase.count(noun) for phrase in wordlist])\n",
    "    \n",
    "    # Find the most frequent noun\n",
    "    main_noun = max(count_nouns, key=count_nouns.get)\n",
    "    \n",
    "    return main_noun\n",
    "\n",
    "main_noun = get_main_nouns(wordlist)\n",
    "\n",
    "print(main_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "730ca289-69ff-44e2-93df-7cf49b283bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective treatments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\"\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases\n",
    "\n",
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    tokens = word_tokenize(words)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    filtered_tagged = [word for word in tagged if word[0].lower() not in stopwords_list]\n",
    "    \n",
    "    noun_list = [word[0] for word in filtered_tagged if word[1] == 'NN' or word[1] == 'NNS']\n",
    "    \n",
    "    # Count the frequency of each noun in the wordlist\n",
    "    count_nouns = {}\n",
    "    for noun in noun_list:\n",
    "        count_nouns[noun] = sum([phrase.count(noun) for phrase in wordlist])\n",
    "    \n",
    "    # Find the most frequent noun\n",
    "    main_noun = max(count_nouns, key=count_nouns.get)\n",
    "    \n",
    "    # Find the noun phrase that contains the main noun\n",
    "    main_noun_phrase = ''\n",
    "    for phrase in wordlist:\n",
    "        if main_noun in phrase:\n",
    "            main_noun_phrase = phrase\n",
    "            break\n",
    "\n",
    "    return main_noun_phrase\n",
    "\n",
    "wordlist = get_main_topic(question)\n",
    "main_noun_phrase = get_main_nouns(wordlist)\n",
    "\n",
    "print(main_noun_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a53be189-e845-41f6-af38-eeaca3c4ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"gensim==3.8.3\"\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7b7e656-7daa-47f5-9fb6-47d19f3f3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a110f1f-9a48-4916-a842-87b4a99cbd59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a84cc5f0-8e71-45fd-b8d7-58b9f7489d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_matutils',\n",
       " 'corpora',\n",
       " 'interfaces',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'matutils',\n",
       " 'models',\n",
       " 'parsing',\n",
       " 'similarities',\n",
       " 'topic_coherence',\n",
       " 'utils']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84050be6-6139-4c51-8f80-d643a234cca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClippedCorpus',\n",
       " 'FakeDict',\n",
       " 'InputQueue',\n",
       " 'NO_CYTHON',\n",
       " 'PAT_ALPHABETIC',\n",
       " 'PICKLE_PROTOCOL',\n",
       " 'RE_HTML_ENTITY',\n",
       " 'RULE_DEFAULT',\n",
       " 'RULE_DISCARD',\n",
       " 'RULE_KEEP',\n",
       " 'RepeatCorpus',\n",
       " 'RepeatCorpusNTimes',\n",
       " 'SaveLoad',\n",
       " 'SlicedCorpus',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_iter_windows',\n",
       " '_pickle',\n",
       " 'any2unicode',\n",
       " 'any2utf8',\n",
       " 'call_on_class_only',\n",
       " 'check_output',\n",
       " 'chunkize',\n",
       " 'chunkize_serial',\n",
       " 'collections',\n",
       " 'contextmanager',\n",
       " 'copytree_hardlink',\n",
       " 'datetime',\n",
       " 'deaccent',\n",
       " 'decode_htmlentities',\n",
       " 'deepcopy',\n",
       " 'default_prng',\n",
       " 'deprecated',\n",
       " 'dict_from_corpus',\n",
       " 'effective_n_jobs',\n",
       " 'file_or_filename',\n",
       " 'flatten',\n",
       " 'gensim_version',\n",
       " 'getNS',\n",
       " 'get_max_id',\n",
       " 'get_my_ip',\n",
       " 'get_random_state',\n",
       " 'grouper',\n",
       " 'heapq',\n",
       " 'identity',\n",
       " 'ignore_deprecation_warning',\n",
       " 'inspect',\n",
       " 'is_corpus',\n",
       " 'is_empty',\n",
       " 'iter_windows',\n",
       " 'itertools',\n",
       " 'keep_vocab_item',\n",
       " 'lazy_flatten',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'merge_counts',\n",
       " 'mock_data',\n",
       " 'mock_data_row',\n",
       " 'multiprocessing',\n",
       " 'n2cp',\n",
       " 'np',\n",
       " 'numbers',\n",
       " 'open',\n",
       " 'open_file',\n",
       " 'os',\n",
       " 'pickle',\n",
       " 'platform',\n",
       " 'prune_vocab',\n",
       " 'pyro_daemon',\n",
       " 'qsize',\n",
       " 'randfname',\n",
       " 'random',\n",
       " 're',\n",
       " 'revdict',\n",
       " 'safe_unichr',\n",
       " 'sample_dict',\n",
       " 'save_as_line_sentence',\n",
       " 'scipy',\n",
       " 'shutil',\n",
       " 'simple_preprocess',\n",
       " 'simple_tokenize',\n",
       " 'smart_extension',\n",
       " 'strided_windows',\n",
       " 'subprocess',\n",
       " 'synchronous',\n",
       " 'sys',\n",
       " 'tempfile',\n",
       " 'to_unicode',\n",
       " 'to_utf8',\n",
       " 'tokenize',\n",
       " 'toptexts',\n",
       " 'trim_vocab_by_freq',\n",
       " 'types',\n",
       " 'unicodedata',\n",
       " 'unpickle',\n",
       " 'upload_chunked',\n",
       " 'warnings',\n",
       " 'with_statement',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gensim.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3de78d4-d558-4ec1-b4e1-73bc63033c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keywords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06178417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "#from gensim.summarization import keywords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\"\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca11e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "#from gensim.summarization import keywords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "question = \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\"\n",
    "\n",
    "def get_main_topic(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.noun_phrases\n",
    "\n",
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    \n",
    "    # Apply TextRank to get the most significant keyword\n",
    "    #main_keyword = keywords(words, words=1, lemmatize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b110d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_most_significant_keyword(list_words):\n",
    "    # Combine all words into a single string\n",
    "    text = ' '.join(list_words)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(filtered_words)\n",
    "\n",
    "    # Get the most frequent word\n",
    "    most_significant_keyword = fdist.most_common(1)[0][0]\n",
    "\n",
    "    return most_significant_keyword\n",
    "\n",
    "list_words = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "most_significant_keyword = get_most_significant_keyword(list_words)\n",
    "print(most_significant_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d284abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f7af1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_most_significant_keyword(list_words):\n",
    "    # Combine all words into a single string\n",
    "    text = ' '.join(list_words)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_words = []\n",
    "    for sentence in words:\n",
    "        filtered_words.extend([lemmatizer.lemmatize(word.lower()) for word in sentence if word.lower() not in stop_words])\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(filtered_words)\n",
    "\n",
    "    # Get the most frequent word that is not a stopword\n",
    "    most_significant_keyword = None\n",
    "    for word, frequency in fdist.most_common():\n",
    "        if word not in stop_words:\n",
    "            most_significant_keyword = word\n",
    "            break\n",
    "\n",
    "    return most_significant_keyword\n",
    "\n",
    "list_words = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "most_significant_keyword = get_most_significant_keyword(list_words)\n",
    "print(most_significant_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61fcf6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant keyword for Arxiv search is: effective treatments\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample list of keywords\n",
    "keywords = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "# Download NLTK words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a word frequency distribution based on the NLTK words corpus\n",
    "word_corpus = set(words.words())\n",
    "fdist = FreqDist(word.lower() for word in word_corpus)\n",
    "\n",
    "# Function to calculate relevance of a keyword\n",
    "def calculate_relevance(keyword):\n",
    "    return fdist[keyword.lower()]\n",
    "\n",
    "# Find the most relevant keyword from the list\n",
    "most_relevant_keyword = max(keywords, key=calculate_relevance)\n",
    "\n",
    "# Print the most relevant keyword\n",
    "print(\"The most relevant keyword for Arxiv search is:\", most_relevant_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82be273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "065736de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = get_main_topic(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8740eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['effective treatments', 'cardiovascular diseases', 'patients worldwide'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "743c52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_noun_phrase = get_main_nouns(wordlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aaf763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601b7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e069f7c-0c36-4e3e-8d10-4eef3b568180",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main_keyword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m main_noun_phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m wordlist:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmain_keyword\u001b[49m \u001b[38;5;129;01min\u001b[39;00m phrase:\n\u001b[0;32m      5\u001b[0m         main_noun_phrase \u001b[38;5;241m=\u001b[39m phrase\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main_keyword' is not defined"
     ]
    }
   ],
   "source": [
    "    # Find the noun phrase that contains the main keyword\n",
    "    main_noun_phrase = ''\n",
    "    for phrase in wordlist:\n",
    "        if main_keyword in phrase:\n",
    "            main_noun_phrase = phrase\n",
    "            break\n",
    "\n",
    "    return main_noun_phrase\n",
    "\n",
    "wordlist = get_main_topic(question)\n",
    "main_noun_phrase = get_main_nouns(wordlist)\n",
    "\n",
    "print(main_noun_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74523d16-d3ff-4edd-a910-ae9e45bfb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_most_significant_keyword(list_words):\n",
    "    # Combine all words into a single string\n",
    "    text = ' '.join(list_words)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_words = []\n",
    "    for sentence in words:\n",
    "        filtered_words.extend([lemmatizer.lemmatize(word.lower()) for word in sentence if word.lower() not in stop_words])\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(filtered_words)\n",
    "\n",
    "    # Get the most frequent word\n",
    "    most_significant_keyword = fdist.most_common(1)[0][0]\n",
    "\n",
    "    return most_significant_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8353055b-1858-4c0d-bd3f-4af5d62d046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words=['effective treatments', 'cardiovascular diseases', 'patients worldwide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30181b53-1e7e-4c0f-a6eb-13b03f06b68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effective'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_significant_keyword(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f838fd3-2917-4b53-a9e0-52209ca96a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2a585a7-eda0-4d63-b0f8-45cf8304b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective treatments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wordlist = ['effective treatments', 'cardiovascular diseases', 'patients worldwide']\n",
    "\n",
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    tokens = word_tokenize(words)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    filtered_tagged = [word for word in tagged if word[0].lower() not in stopwords_list]\n",
    "    \n",
    "    noun_list = [word[0] for word in filtered_tagged if word[1] == 'NN' or word[1] == 'NNS']\n",
    "    \n",
    "    # Count the frequency of each noun in the wordlist\n",
    "    count_nouns = {}\n",
    "    for noun in noun_list:\n",
    "        count_nouns[noun] = sum([phrase.count(noun) for phrase in wordlist])\n",
    "    \n",
    "    # Find the most frequent noun\n",
    "    main_noun = max(count_nouns, key=count_nouns.get)\n",
    "    \n",
    "    # Find the noun phrase that contains the main noun\n",
    "    main_noun_phrase = ''\n",
    "    for phrase in wordlist:\n",
    "        if main_noun in phrase:\n",
    "            main_noun_phrase = phrase\n",
    "            break\n",
    "\n",
    "    return main_noun_phrase\n",
    "\n",
    "main_noun_phrase = get_main_nouns(wordlist)\n",
    "\n",
    "print(main_noun_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff7270-d8c8-4521-b89f-9d32014e20d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9e80b-2c32-473e-8c9b-b7d7616dcf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069748d-4b66-45c1-9df7-7460cfd2b87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4b04f-815e-4b91-98a4-7fa70ee29527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d4bcaa9-3420-4a0e-91c8-ba42be29c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_nouns(wordlist):\n",
    "    words = ' '.join(wordlist)\n",
    "    tokens = word_tokenize(words)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    filtered_tagged = [word for word in tagged if word[0].lower() not in stopwords_list]\n",
    "    \n",
    "    noun_list = [word[0] for word in filtered_tagged if word[1] == 'NN' or word[1] == 'NNS']\n",
    "    \n",
    "    count_nouns = Counter(noun_list)\n",
    "    main_noun = count_nouns.most_common(1)[0][0]\n",
    "    \n",
    "    # Find the noun phrase that contains the main noun\n",
    "    main_noun_phrase = ''\n",
    "    for phrase in wordlist:\n",
    "        if main_noun in phrase:\n",
    "            main_noun_phrase = phrase\n",
    "            break\n",
    "\n",
    "    return main_noun_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4159d8e-e96c-490e-8a20-883cc5d05dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_noun_phrase = get_main_nouns(wordlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "883c6049-b7b3-4894-b539-2557b04bd499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective treatments\n"
     ]
    }
   ],
   "source": [
    "print(main_noun_phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ccffa-26a7-4381-84b1-02a9b36fa704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dca420-dbf2-4f77-9589-5df9c6c32ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fa6ba-73db-4a13-a587-b75f022136ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ddf6f-6b3d-4eec-98b8-a209e9aea5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38bb400-9ff5-4bf6-8b92-ff8157427d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e8b9d2-d528-4173-adcf-9e36ac93c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e48395-d07f-49e7-989e-ba55ffc7c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get question\n",
    "question = 'What are the current therapies with Tinnitus?'\n",
    "# question = \"Can't do this shit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58419921-9f40-4d06-9817-46f50db75137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\066226758\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f672f48-34d0-4f57-b391-b6b9b3c66fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\066226758\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\066226758\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\066226758\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Search terms cannot start with a wildcard (? *).\n",
    "# rimuovere punteggiatura\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812aaa91-d7ee-4492-a5eb-aeb2ebc9c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['What', 'are', 'the', 'current', 'therapies', 'with', 'Tinnitus', '?']\n"
     ]
    }
   ],
   "source": [
    "# transform question into topics via NLTK (preprocessing?)\n",
    "# tokenization\n",
    "stop_words = set(stopwords.words('english')) # elenco di parole prive di significato\n",
    "word_tokens = word_tokenize(question) # tokenizzazione\n",
    "print('tokens: {}'.format(word_tokens))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7845b5d-fb64-4e25-897e-457d966a1ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered tokens: ['current', 'therapy', 'Tinnitus']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words and lemmatize tokens\n",
    "filtered_tokens = [lemmatizer.lemmatize(w) for w in word_tokens if w.lower() not in stop_words if w.isalnum()]\n",
    "print('filtered tokens: {}'.format(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a400849-be83-4139-9d42-a5430a12f54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07b6b3be-6470-49d7-9511-f60d39abfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(sentence):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    nouns = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "            nouns.append(word)\n",
    "\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d58cdc-7c5b-4570-ae78-6f465fc997b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = extract_nouns(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69aa80ab-46bb-439b-812d-292819e67cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['therapies', 'Tinnitus']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9678ff-375b-4559-a14f-ba06f3e0bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_noun(sentence):\n",
    "    nouns = extract_nouns(sentence)\n",
    "    return nouns[-1] if nouns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0e9a0f-9889-4056-986f-3e717136b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tinnitus\n"
     ]
    }
   ],
   "source": [
    "main_keyword = main_noun(question)\n",
    "print(main_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b4babb-6a4e-49c8-909a-5778a6e9fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\",\n",
    "    \"How can early detection and prevention strategies for cancer be improved and implemented on a global scale?\",\n",
    "    \"What are the key factors contributing to the rise of diabetes, and how can lifestyle interventions be used to combat this epidemic?\",\n",
    "    \"How can we develop more effective vaccines and treatments for respiratory infections such as pneumonia and influenza?\",\n",
    "    \"What are the most promising advances in Alzheimer's disease research, and how can these findings be translated into clinical practice?\",\n",
    "    \"What strategies can be employed to prevent and manage the growing global burden of chronic kidney disease?\",\n",
    "    \"How can we improve the understanding of mental health disorders, such as depression and anxiety, to develop more effective therapies?\",\n",
    "    \"What are the most significant challenges in eradicating malaria, and how can we overcome them?\",\n",
    "    \"How can we improve access to HIV/AIDS treatment and prevention methods in regions with high prevalence rates?\",\n",
    "    \"What are the most effective ways to prevent and treat malnutrition in children and adults worldwide?\",\n",
    "    \"How can we address the global rise in antibiotic resistance, and what alternative treatments can be developed for bacterial infections?\",\n",
    "    \"What are the key factors driving the obesity epidemic, and how can public health interventions help reverse this trend?\",\n",
    "    \"How can we improve our understanding of the genetic and environmental factors contributing to autoimmune diseases such as lupus and rheumatoid arthritis?\",\n",
    "    \"What are the most promising areas of research for developing new treatments for chronic pain conditions?\",\n",
    "    \"How can we better understand and manage the global burden of neurological disorders, such as multiple sclerosis and Parkinson's disease?\",\n",
    "    \"What are the most effective strategies for reducing the impact of substance abuse and addiction on individuals and communities?\",\n",
    "    \"How can we improve the early detection and treatment of rare genetic disorders, such as cystic fibrosis and muscular dystrophy?\",\n",
    "    \"What are the most significant challenges in combating neglected tropical diseases, and how can we address these issues?\",\n",
    "    \"How can we develop more effective interventions for preventing and treating age-related diseases, such as osteoporosis and macular degeneration?\",\n",
    "    \"What are the most promising areas of research for understanding and treating chronic liver diseases, including hepatitis and cirrhosis?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b645de-caee-4add-b304-85247acedc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treatments', 'diseases', 'patients']  What are the most effective treatments for cardiovascular diseases, and how can they be made more accessible to patients worldwide?\n",
      "['detection', 'prevention', 'strategies', 'cancer', 'scale']  How can early detection and prevention strategies for cancer be improved and implemented on a global scale?\n",
      "['factors', 'rise', 'diabetes', 'interventions', 'combat', 'epidemic']  What are the key factors contributing to the rise of diabetes, and how can lifestyle interventions be used to combat this epidemic?\n",
      "['vaccines', 'treatments', 'infections', 'influenza']  How can we develop more effective vaccines and treatments for respiratory infections such as pneumonia and influenza?\n",
      "['advances', 'Alzheimer', 'disease', 'research', 'findings', 'practice']  What are the most promising advances in Alzheimer's disease research, and how can these findings be translated into clinical practice?\n",
      "['strategies', 'manage', 'burden', 'kidney', 'disease']  What strategies can be employed to prevent and manage the growing global burden of chronic kidney disease?\n",
      "['health', 'disorders', 'depression', 'anxiety', 'therapies']  How can we improve the understanding of mental health disorders, such as depression and anxiety, to develop more effective therapies?\n",
      "['challenges', 'malaria']  What are the most significant challenges in eradicating malaria, and how can we overcome them?\n",
      "['access', 'HIV/AIDS', 'treatment', 'prevention', 'methods', 'regions', 'prevalence', 'rates']  How can we improve access to HIV/AIDS treatment and prevention methods in regions with high prevalence rates?\n",
      "['ways', 'treat', 'malnutrition', 'children', 'adults']  What are the most effective ways to prevent and treat malnutrition in children and adults worldwide?\n",
      "['address', 'rise', 'resistance', 'treatments', 'infections']  How can we address the global rise in antibiotic resistance, and what alternative treatments can be developed for bacterial infections?\n",
      "['factors', 'obesity', 'health', 'interventions', 'trend']  What are the key factors driving the obesity epidemic, and how can public health interventions help reverse this trend?\n",
      "['understanding', 'factors', 'autoimmune', 'diseases', 'arthritis']  How can we improve our understanding of the genetic and environmental factors contributing to autoimmune diseases such as lupus and rheumatoid arthritis?\n",
      "['areas', 'research', 'treatments', 'pain', 'conditions']  What are the most promising areas of research for developing new treatments for chronic pain conditions?\n",
      "['manage', 'burden', 'disorders', 'sclerosis', 'Parkinson', 'disease']  How can we better understand and manage the global burden of neurological disorders, such as multiple sclerosis and Parkinson's disease?\n",
      "['strategies', 'substance', 'abuse', 'addiction', 'individuals', 'communities']  What are the most effective strategies for reducing the impact of substance abuse and addiction on individuals and communities?\n",
      "['detection', 'treatment', 'disorders', 'fibrosis', 'muscular', 'dystrophy']  How can we improve the early detection and treatment of rare genetic disorders, such as cystic fibrosis and muscular dystrophy?\n",
      "['challenges', 'diseases', 'address', 'issues']  What are the most significant challenges in combating neglected tropical diseases, and how can we address these issues?\n",
      "['interventions', 'diseases', 'osteoporosis', 'degeneration']  How can we develop more effective interventions for preventing and treating age-related diseases, such as osteoporosis and macular degeneration?\n",
      "['areas', 'research', 'liver', 'diseases', 'hepatitis', 'cirrhosis']  What are the most promising areas of research for understanding and treating chronic liver diseases, including hepatitis and cirrhosis?\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "\n",
    "    output = extract_nouns(question)\n",
    "    main_keyword = main_noun(question)\n",
    "    print(output, \"\" , question)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c229bf-5e73-44d9-99ee-2d837c84da87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9540198-209d-441a-b04c-90a92b171cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fcba7-cc95-4a0b-ac9b-bbfd58cb7cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342607a-fafb-4b03-8a8f-9e4b4d0d8f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4344ad8-3175-4b9a-b866-20fcc600c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full topic creation\n",
    "topic = ' OR '.join(filtered_tokens)\n",
    "\n",
    "# combinations of single topics\n",
    "\n",
    "subtopics = list()\n",
    "titles = list()\n",
    "authors = list()\n",
    "summary = list()\n",
    "pdf_url = list()\n",
    "\n",
    "for subtopic in filtered_tokens:\n",
    "    search = arxiv.Search(\n",
    "        query=subtopic,\n",
    "        max_results=10,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    print('Fetching items for token: {}'.format(subtopic))\n",
    "    for result in arxiv.Client().results(search):\n",
    "        subtopics.append(subtopic)\n",
    "        titles.append([result.title for result in arxiv.Client().results(search)])\n",
    "        authors.append([result.authors for result in arxiv.Client().results(search)])\n",
    "        summary.append([result.summary for result in arxiv.Client().results(search)])\n",
    "        pdf_url.append([result.pdf_url for result in arxiv.Client().results(search)])\n",
    "\n",
    "print('Fetch completed.')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "      'subtopic': subtopics,\n",
    "      'title': titles,\n",
    "      'authors': authors,\n",
    "      'summary': summary,\n",
    "      'pdf_url': pdf_url\n",
    " })\n",
    "\n",
    "df.to_excel(r\"C:\\Users\\066271758\\Desktop\\Progetti\\WatsonX\\arxiv\\ArxivQuery\\test.xlsx\")\n",
    "print('Excel file created. Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ArxivChat)",
   "language": "python",
   "name": "arxivchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
