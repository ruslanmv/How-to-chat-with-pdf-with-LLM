{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1c5caa",
   "metadata": {},
   "source": [
    "# Environment test for WatsonX.ai  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473b1895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to create bearer\n",
      "Bearer retrieved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import TensorflowHubEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "    GenParams.MIN_NEW_TOKENS: 0,\n",
    "    GenParams.STOP_SEQUENCES: [\"\\n\"],\n",
    "    GenParams.REPETITION_PENALTY:1\n",
    "    }\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "        \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "        }    \n",
    "#this cell should never fail, and will produce no output\n",
    "import requests\n",
    "\n",
    "def getBearer(apikey):\n",
    "    form = {'apikey': apikey, 'grant_type': \"urn:ibm:params:oauth:grant-type:apikey\"}\n",
    "    print(\"About to create bearer\")\n",
    "#    print(form)\n",
    "    response = requests.post(\"https://iam.cloud.ibm.com/oidc/token\", data = form)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Bad response code retrieving token\")\n",
    "        raise Exception(\"Failed to get token, invalid status\")\n",
    "    json = response.json()\n",
    "    if not json:\n",
    "        print(\"Invalid/no JSON retrieving token\")\n",
    "        raise Exception(\"Failed to get token, invalid response\")\n",
    "    print(\"Bearer retrieved\")\n",
    "    return json.get(\"access_token\")\n",
    "\n",
    "credentials[\"token\"] = getBearer(credentials[\"apikey\"])\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "model_id = ModelTypes.LLAMA_2_70B_CHAT\n",
    "\n",
    "# Initialize the Watsonx foundation model\n",
    "llama_model = Model(\n",
    "    model_id=model_id, \n",
    "    params=parameters, \n",
    "    credentials=credentials,\n",
    "    project_id=project_id)\n",
    "\n",
    "\n",
    "# Function to get text from PDF documents\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        text += \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "    return text\n",
    "\n",
    "# Function to split text into chunks\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Function to create a vector store\n",
    "def get_vectorstore(text_chunks):\n",
    "    url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "    #embeddings = HuggingFaceInstructEmbeddings() \n",
    "    embeddings  = TensorflowHubEmbeddings(model_url=url)   \n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Function to create a conversation chain\n",
    "def get_conversation_chain(vectorstore):\n",
    "    #llm = ChatOpenAI()\n",
    "    #llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "    llm=llama_model.to_langchain()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6f133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current working directory\n",
    "current_working_directory = os.getcwd()# Path\n",
    "# Join various path components \n",
    "pdf_path=os.path.join(current_working_directory, \"documents\", \"example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71758fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF text and split into chunks\n",
    "raw_text = get_pdf_text([pdf_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c6e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = get_text_chunks(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1e1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store and conversation chain\n",
    "vectorstore = get_vectorstore(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb68a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=llama_model.to_langchain()\n",
    "memory = ConversationBufferMemory(memory_key='chat_history',\n",
    "                                    return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e192c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3234e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the topic about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8956ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt={\"question\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "591caad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversation_chain(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54c5543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the topic about',\n",
       " 'chat_history': [HumanMessage(content='Instruction: Answer the following question using only information from the article. Question: What is the topic about', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" The article discusses the Transformer model, a new neural network architecture for natural language processing tasks. It describes the model's attention mechanism, which allows it to jointly attend to information from different representation subspaces at different positions. The article also discusses the training regime for the model, including the training data and batching.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Instruction: Answer the following question using only information from the article.If there is no good answer in the article, say \"I don\\'t know\". Question: What is the topic about', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='  The Transformer model is primarily used for sequence transduction, specifically for tasks such as machine translation and text summarization. It is designed to compute representations of input and output sequences using self-attention mechanisms, rather than relying on sequence-aligned RNNs or convolutions.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Instruction: Answer the following question using only information from the article.If there is no good answer in the article, say \"I don\\'t know\". Question: What is the temperature in Genova', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"  I don't know.\\n\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the topic about', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='  The article discusses the Transformer model, a new architecture for neural machine translation that relies entirely on self-attention mechanisms to compute representations of its input and output.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the temperature in Genova', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"  I don't know.\\n\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the topic about', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='  The article discusses the Transformer model, a new architecture for neural machine translation that relies entirely on self-attention mechanisms to compute representations of its input and output.', additional_kwargs={}, example=False)],\n",
       " 'answer': '  The article discusses the Transformer model, a new architecture for neural machine translation that relies entirely on self-attention mechanisms to compute representations of its input and output.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2ccee",
   "metadata": {},
   "source": [
    "\n",
    "To include instructions to the large language model in the python code above, you can use the **parameters** argument when calling the conversation_chain.generate() method. This argument is a dictionary that can contain any additional parameters that you want to pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "117092a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"instruction\": \"Answer the following question using only information from the article. If there is no good answer in the article, say I don't know\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "effdaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the topic about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff29beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt={\"question\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e760eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(conversation_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73185b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversation_chain(prompt, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e3119a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '  The article discusses the Transformer model, a new architecture for neural machine translation that relies entirely on self-attention mechanisms to compute representations of its input and output.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7a238d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the temperature in Genova\"\n",
    "prompt={\"question\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26f1c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversation_chain(prompt,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddfea820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"  I don't know.\\n\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc1095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Instruction: Answer the following question using only information from the article. Question: What is the topic about', additional_kwargs={}, example=False), AIMessage(content=\" The article discusses the Transformer model, a new neural network architecture for natural language processing tasks. It describes the model's attention mechanism, which allows it to jointly attend to information from different representation subspaces at different positions. The article also discusses the training regime for the model, including the training data and batching.\", additional_kwargs={}, example=False), HumanMessage(content='Instruction: Answer the following question using only information from the article.If there is no good answer in the article, say \"I don\\'t know\". Question: What is the topic about', additional_kwargs={}, example=False), AIMessage(content='  The Transformer model is primarily used for sequence transduction, specifically for tasks such as machine translation and text summarization. It is designed to compute representations of input and output sequences using self-attention mechanisms, rather than relying on sequence-aligned RNNs or convolutions.', additional_kwargs={}, example=False), HumanMessage(content='Instruction: Answer the following question using only information from the article.If there is no good answer in the article, say \"I don\\'t know\". Question: What is the temperature in Genova', additional_kwargs={}, example=False), AIMessage(content=\"  I don't know.\\n\", additional_kwargs={}, example=False), HumanMessage(content='What is the topic about', additional_kwargs={}, example=False), AIMessage(content='  The article discusses the Transformer model, a new architecture for neural machine translation that relies entirely on self-attention mechanisms to compute representations of its input and output.', additional_kwargs={}, example=False), HumanMessage(content='What is the temperature in Genova', additional_kwargs={}, example=False), AIMessage(content=\"  I don't know.\\n\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54454fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatpdf)",
   "language": "python",
   "name": "chatpdf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
